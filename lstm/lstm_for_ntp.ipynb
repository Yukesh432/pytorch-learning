{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rNVVsTddrrVh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import nltk\n",
        "\n",
        "# Function to load data from a file\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenize_text(text, use_nltk=False):\n",
        "    if use_nltk:\n",
        "        nltk.download('punkt')\n",
        "        return nltk.word_tokenize(text)\n",
        "    else:\n",
        "        return text.split()\n",
        "\n",
        "# Function to build a vocabulary from tokens\n",
        "def build_vocab(tokens):\n",
        "    vocab = set(tokens)\n",
        "    token_to_index = {token: idx for idx, token in enumerate(vocab)}\n",
        "    index_to_token = {idx: token for token, idx in token_to_index.items()}\n",
        "    return token_to_index, index_to_token\n",
        "\n",
        "# Function to convert tokens to indices\n",
        "def tokens_to_indices(tokens, token_to_index):\n",
        "    return [token_to_index[token] for token in tokens]\n",
        "\n",
        "# Function to create sequences for training\n",
        "def create_sequences(token_indices, seq_length):\n",
        "    sequences = [token_indices[i:i + seq_length] for i in range(len(token_indices) - seq_length)]\n",
        "    return sequences\n",
        "\n",
        "# Function to prepare training data\n",
        "def prepare_training_data(sequences, seq_length):\n",
        "    # Ensure that inputs and targets are of the same length\n",
        "    inputs = [sequence[:-1] for sequence in sequences]  # Exclude the last token for input\n",
        "    targets = [sequence[1:] for sequence in sequences]  # Exclude the first token for target\n",
        "    return inputs, targets\n",
        "\n",
        "# Function to convert data to PyTorch tensors\n",
        "def to_tensors(inputs, targets):\n",
        "    input_tensors = torch.tensor(inputs, dtype=torch.long)\n",
        "    target_tensors = torch.tensor(targets, dtype=torch.long)\n",
        "    return input_tensors, target_tensors\n",
        "\n",
        "# LSTM Model Definition\n",
        "class TokenLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1):\n",
        "        super(TokenLSTM, self).__init__()\n",
        "        self.vocab_size= vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, sequences):\n",
        "        embedded = self.embedding(sequences)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        out = self.linear(lstm_out)\n",
        "        return out\n",
        "\n",
        "# Function to Initialize the Model\n",
        "def init_model(vocab_size, embedding_dim, hidden_size, num_layers=1):\n",
        "    model = TokenLSTM(vocab_size, embedding_dim, hidden_size, num_layers)\n",
        "    return model\n",
        "\n",
        "# Function to Create DataLoader\n",
        "def create_data_loader(input_tensors, target_tensors, batch_size=64):\n",
        "    dataset = TensorDataset(input_tensors, target_tensors)\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return data_loader\n",
        "\n",
        "def train_model(model, data_loader, learning_rate, num_epochs, device):\n",
        "    print(\"Entering train_model function....\")\n",
        "    model.to(device)  # Move the model to the specified device (GPU/CPU)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        total_loss = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to device\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.view(-1, model.vocab_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 100 == 0:  # Print progress every 100 batches\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item()}')\n",
        "\n",
        "        average_loss = total_loss / len(data_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on: {device}\")\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/wiki.train.raw'\n",
        "text_data = load_data(file_path)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenize_text(text_data, use_nltk=True)\n",
        "\n",
        "# Build the vocabulary\n",
        "token_to_index, index_to_token = build_vocab(tokens)\n",
        "\n",
        "# Convert tokens to indices\n",
        "token_indices = tokens_to_indices(tokens, token_to_index)\n",
        "\n",
        "# Define the sequence length\n",
        "seq_length = 30\n",
        "\n",
        "# Create sequences\n",
        "sequences = create_sequences(token_indices, seq_length)\n",
        "\n",
        "# Prepare training data\n",
        "inputs, targets = prepare_training_data(sequences, seq_length)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "input_tensors, target_tensors = to_tensors(inputs, targets)\n",
        "\n",
        "# Print tensor shapes for verification\n",
        "print(input_tensors.shape)\n",
        "print(target_tensors.shape)\n",
        "print(\"......................................................................\")\n",
        "\n",
        "# Model Hyperparameters\n",
        "vocab_size = len(token_to_index)  # Length of the vocabulary\n",
        "embedding_dim = 256\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "batch_size = 128\n",
        "# Initialize the Model\n",
        "lstm_model = init_model(vocab_size, embedding_dim, hidden_size, num_layers)\n",
        "\n",
        "print('model defined.....................')\n",
        "\n",
        "# Create DataLoader\n",
        "data_loader = create_data_loader(input_tensors, target_tensors, batch_size)\n",
        "\n",
        "print(\"Training Starts......................................\")\n",
        "# Train the Model\n",
        "train_model(lstm_model, data_loader, learning_rate, num_epochs, device)\n",
        "\n",
        "# Saving the model parameters\n",
        "model_save_path = 'trained_lstm_model.pth'\n",
        "torch.save(lstm_model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5QQDqvhsko1",
        "outputId": "75ab720f-ce19-4866-bace-d31406d3f6a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2098745, 29])\n",
            "torch.Size([2098745, 29])\n",
            "......................................................................\n",
            "model defined.....................\n",
            "Training Starts......................................\n",
            "Entering train_model function....\n",
            "Epoch 1/5, Batch 0, Loss: 11.243497848510742\n",
            "Epoch 1/5, Batch 100, Loss: 7.512975215911865\n",
            "Epoch 1/5, Batch 200, Loss: 7.55238151550293\n",
            "Epoch 1/5, Batch 300, Loss: 7.330526351928711\n",
            "Epoch 1/5, Batch 400, Loss: 7.219995975494385\n",
            "Epoch 1/5, Batch 500, Loss: 7.007105350494385\n",
            "Epoch 1/5, Batch 600, Loss: 6.93166446685791\n",
            "Epoch 1/5, Batch 700, Loss: 6.930108547210693\n",
            "Epoch 1/5, Batch 800, Loss: 6.726434707641602\n",
            "Epoch 1/5, Batch 900, Loss: 6.649255752563477\n",
            "Epoch 1/5, Batch 1000, Loss: 6.682747840881348\n",
            "Epoch 1/5, Batch 1100, Loss: 6.574718952178955\n",
            "Epoch 1/5, Batch 1200, Loss: 6.618342876434326\n",
            "Epoch 1/5, Batch 1300, Loss: 6.396847248077393\n",
            "Epoch 1/5, Batch 1400, Loss: 6.353071212768555\n",
            "Epoch 1/5, Batch 1500, Loss: 6.446681022644043\n",
            "Epoch 1/5, Batch 1600, Loss: 6.395687580108643\n",
            "Epoch 1/5, Batch 1700, Loss: 6.191202640533447\n",
            "Epoch 1/5, Batch 1800, Loss: 6.208303928375244\n",
            "Epoch 1/5, Batch 1900, Loss: 6.212525367736816\n",
            "Epoch 1/5, Batch 2000, Loss: 6.232136249542236\n",
            "Epoch 1/5, Batch 2100, Loss: 6.168793201446533\n",
            "Epoch 1/5, Batch 2200, Loss: 6.06745719909668\n",
            "Epoch 1/5, Batch 2300, Loss: 6.203420162200928\n",
            "Epoch 1/5, Batch 2400, Loss: 5.950129508972168\n",
            "Epoch 1/5, Batch 2500, Loss: 6.039150714874268\n",
            "Epoch 1/5, Batch 2600, Loss: 6.080479621887207\n",
            "Epoch 1/5, Batch 2700, Loss: 6.087050437927246\n",
            "Epoch 1/5, Batch 2800, Loss: 5.958443641662598\n",
            "Epoch 1/5, Batch 2900, Loss: 5.846805095672607\n",
            "Epoch 1/5, Batch 3000, Loss: 5.994884490966797\n",
            "Epoch 1/5, Batch 3100, Loss: 5.927451133728027\n",
            "Epoch 1/5, Batch 3200, Loss: 5.836481094360352\n",
            "Epoch 1/5, Batch 3300, Loss: 5.843184947967529\n",
            "Epoch 1/5, Batch 3400, Loss: 5.746037006378174\n",
            "Epoch 1/5, Batch 3500, Loss: 5.848089694976807\n",
            "Epoch 1/5, Batch 3600, Loss: 5.81565523147583\n",
            "Epoch 1/5, Batch 3700, Loss: 5.804041385650635\n",
            "Epoch 1/5, Batch 3800, Loss: 5.817657947540283\n",
            "Epoch 1/5, Batch 3900, Loss: 5.6878204345703125\n",
            "Epoch 1/5, Batch 4000, Loss: 5.644641876220703\n",
            "Epoch 1/5, Batch 4100, Loss: 5.475444316864014\n",
            "Epoch 1/5, Batch 4200, Loss: 5.598247528076172\n",
            "Epoch 1/5, Batch 4300, Loss: 5.4812846183776855\n",
            "Epoch 1/5, Batch 4400, Loss: 5.550726890563965\n",
            "Epoch 1/5, Batch 4500, Loss: 5.505302906036377\n",
            "Epoch 1/5, Batch 4600, Loss: 5.396665573120117\n",
            "Epoch 1/5, Batch 4700, Loss: 5.43065881729126\n",
            "Epoch 1/5, Batch 4800, Loss: 5.512837886810303\n",
            "Epoch 1/5, Batch 4900, Loss: 5.373549461364746\n",
            "Epoch 1/5, Batch 5000, Loss: 5.503742218017578\n",
            "Epoch 1/5, Batch 5100, Loss: 5.477657318115234\n",
            "Epoch 1/5, Batch 5200, Loss: 5.383870601654053\n",
            "Epoch 1/5, Batch 5300, Loss: 5.211941719055176\n",
            "Epoch 1/5, Batch 5400, Loss: 5.2707319259643555\n",
            "Epoch 1/5, Batch 5500, Loss: 5.401785373687744\n",
            "Epoch 1/5, Batch 5600, Loss: 5.166306018829346\n",
            "Epoch 1/5, Batch 5700, Loss: 5.226535797119141\n",
            "Epoch 1/5, Batch 5800, Loss: 5.1796345710754395\n",
            "Epoch 1/5, Batch 5900, Loss: 5.258098602294922\n",
            "Epoch 1/5, Batch 6000, Loss: 5.186131477355957\n",
            "Epoch 1/5, Batch 6100, Loss: 5.122109889984131\n",
            "Epoch 1/5, Batch 6200, Loss: 5.1113739013671875\n",
            "Epoch 1/5, Batch 6300, Loss: 5.069153785705566\n",
            "Epoch 1/5, Batch 6400, Loss: 5.030874252319336\n",
            "Epoch 1/5, Batch 6500, Loss: 4.977860927581787\n",
            "Epoch 1/5, Batch 6600, Loss: 4.890950679779053\n",
            "Epoch 1/5, Batch 6700, Loss: 4.995809555053711\n",
            "Epoch 1/5, Batch 6800, Loss: 5.022154808044434\n",
            "Epoch 1/5, Batch 6900, Loss: 5.06036376953125\n",
            "Epoch 1/5, Batch 7000, Loss: 4.867403984069824\n",
            "Epoch 1/5, Batch 7100, Loss: 4.972183704376221\n",
            "Epoch 1/5, Batch 7200, Loss: 4.859480381011963\n",
            "Epoch 1/5, Batch 7300, Loss: 4.97294282913208\n",
            "Epoch 1/5, Batch 7400, Loss: 4.861996173858643\n",
            "Epoch 1/5, Batch 7500, Loss: 4.948154449462891\n",
            "Epoch 1/5, Batch 7600, Loss: 4.771413326263428\n",
            "Epoch 1/5, Batch 7700, Loss: 4.768558025360107\n",
            "Epoch 1/5, Batch 7800, Loss: 4.784392833709717\n",
            "Epoch 1/5, Batch 7900, Loss: 4.645339012145996\n",
            "Epoch 1/5, Batch 8000, Loss: 4.751708984375\n",
            "Epoch 1/5, Batch 8100, Loss: 4.578594207763672\n",
            "Epoch 1/5, Batch 8200, Loss: 4.652145862579346\n",
            "Epoch 1/5, Batch 8300, Loss: 4.7990498542785645\n",
            "Epoch 1/5, Batch 8400, Loss: 4.697082042694092\n",
            "Epoch 1/5, Batch 8500, Loss: 4.702838897705078\n",
            "Epoch 1/5, Batch 8600, Loss: 4.630865573883057\n",
            "Epoch 1/5, Batch 8700, Loss: 4.580132007598877\n",
            "Epoch 1/5, Batch 8800, Loss: 4.641984939575195\n",
            "Epoch 1/5, Batch 8900, Loss: 4.469603538513184\n",
            "Epoch 1/5, Batch 9000, Loss: 4.60796594619751\n",
            "Epoch 1/5, Batch 9100, Loss: 4.5483479499816895\n",
            "Epoch 1/5, Batch 9200, Loss: 4.524074077606201\n",
            "Epoch 1/5, Batch 9300, Loss: 4.496159076690674\n",
            "Epoch 1/5, Batch 9400, Loss: 4.38722038269043\n",
            "Epoch 1/5, Batch 9500, Loss: 4.512007713317871\n",
            "Epoch 1/5, Batch 9600, Loss: 4.44854736328125\n",
            "Epoch 1/5, Batch 9700, Loss: 4.326440334320068\n",
            "Epoch 1/5, Batch 9800, Loss: 4.535597324371338\n",
            "Epoch 1/5, Batch 9900, Loss: 4.345852851867676\n",
            "Epoch 1/5, Batch 10000, Loss: 4.354915618896484\n",
            "Epoch 1/5, Batch 10100, Loss: 4.4201884269714355\n",
            "Epoch 1/5, Batch 10200, Loss: 4.351766586303711\n",
            "Epoch 1/5, Batch 10300, Loss: 4.299102306365967\n",
            "Epoch 1/5, Batch 10400, Loss: 4.396799564361572\n",
            "Epoch 1/5, Batch 10500, Loss: 4.370213508605957\n",
            "Epoch 1/5, Batch 10600, Loss: 4.43130350112915\n",
            "Epoch 1/5, Batch 10700, Loss: 4.324750900268555\n",
            "Epoch 1/5, Batch 10800, Loss: 4.191305637359619\n",
            "Epoch 1/5, Batch 10900, Loss: 4.236239433288574\n",
            "Epoch 1/5, Batch 11000, Loss: 4.1567277908325195\n",
            "Epoch 1/5, Batch 11100, Loss: 4.32452917098999\n",
            "Epoch 1/5, Batch 11200, Loss: 4.266787528991699\n",
            "Epoch 1/5, Batch 11300, Loss: 4.327342987060547\n",
            "Epoch 1/5, Batch 11400, Loss: 4.194538593292236\n",
            "Epoch 1/5, Batch 11500, Loss: 4.136933326721191\n",
            "Epoch 1/5, Batch 11600, Loss: 4.260753154754639\n",
            "Epoch 1/5, Batch 11700, Loss: 4.06296443939209\n",
            "Epoch 1/5, Batch 11800, Loss: 4.090677261352539\n",
            "Epoch 1/5, Batch 11900, Loss: 4.014203071594238\n",
            "Epoch 1/5, Batch 12000, Loss: 4.084019184112549\n",
            "Epoch 1/5, Batch 12100, Loss: 4.150567054748535\n",
            "Epoch 1/5, Batch 12200, Loss: 4.074749946594238\n",
            "Epoch 1/5, Batch 12300, Loss: 4.0410075187683105\n",
            "Epoch 1/5, Batch 12400, Loss: 4.045252323150635\n",
            "Epoch 1/5, Batch 12500, Loss: 4.0337629318237305\n",
            "Epoch 1/5, Batch 12600, Loss: 4.036067008972168\n",
            "Epoch 1/5, Batch 12700, Loss: 4.062304973602295\n",
            "Epoch 1/5, Batch 12800, Loss: 4.089989185333252\n",
            "Epoch 1/5, Batch 12900, Loss: 4.058404922485352\n",
            "Epoch 1/5, Batch 13000, Loss: 3.940396308898926\n",
            "Epoch 1/5, Batch 13100, Loss: 4.004245281219482\n",
            "Epoch 1/5, Batch 13200, Loss: 3.904385805130005\n",
            "Epoch 1/5, Batch 13300, Loss: 3.893073558807373\n",
            "Epoch 1/5, Batch 13400, Loss: 3.9482526779174805\n",
            "Epoch 1/5, Batch 13500, Loss: 3.9109692573547363\n",
            "Epoch 1/5, Batch 13600, Loss: 3.9890551567077637\n",
            "Epoch 1/5, Batch 13700, Loss: 3.898630142211914\n",
            "Epoch 1/5, Batch 13800, Loss: 3.8836421966552734\n",
            "Epoch 1/5, Batch 13900, Loss: 3.8459367752075195\n",
            "Epoch 1/5, Batch 14000, Loss: 3.8493573665618896\n",
            "Epoch 1/5, Batch 14100, Loss: 3.889434576034546\n",
            "Epoch 1/5, Batch 14200, Loss: 3.9013254642486572\n",
            "Epoch 1/5, Batch 14300, Loss: 3.874720573425293\n",
            "Epoch 1/5, Batch 14400, Loss: 3.8368303775787354\n",
            "Epoch 1/5, Batch 14500, Loss: 3.8659911155700684\n",
            "Epoch 1/5, Batch 14600, Loss: 3.8394062519073486\n",
            "Epoch 1/5, Batch 14700, Loss: 3.8049561977386475\n",
            "Epoch 1/5, Batch 14800, Loss: 3.772984027862549\n",
            "Epoch 1/5, Batch 14900, Loss: 3.8185441493988037\n",
            "Epoch 1/5, Batch 15000, Loss: 3.769606113433838\n",
            "Epoch 1/5, Batch 15100, Loss: 3.743258476257324\n",
            "Epoch 1/5, Batch 15200, Loss: 3.8035078048706055\n",
            "Epoch 1/5, Batch 15300, Loss: 3.7592127323150635\n",
            "Epoch 1/5, Batch 15400, Loss: 3.74530029296875\n",
            "Epoch 1/5, Batch 15500, Loss: 3.791738510131836\n",
            "Epoch 1/5, Batch 15600, Loss: 3.7295427322387695\n",
            "Epoch 1/5, Batch 15700, Loss: 3.666295051574707\n",
            "Epoch 1/5, Batch 15800, Loss: 3.7750184535980225\n",
            "Epoch 1/5, Batch 15900, Loss: 3.7016961574554443\n",
            "Epoch 1/5, Batch 16000, Loss: 3.5612621307373047\n",
            "Epoch 1/5, Batch 16100, Loss: 3.708914279937744\n",
            "Epoch 1/5, Batch 16200, Loss: 3.5880682468414307\n",
            "Epoch 1/5, Batch 16300, Loss: 3.683485507965088\n",
            "Epoch [1/5], Average Loss: 4.914865263367584\n",
            "Epoch 2/5, Batch 0, Loss: 3.428300380706787\n",
            "Epoch 2/5, Batch 100, Loss: 3.5479583740234375\n",
            "Epoch 2/5, Batch 200, Loss: 3.5437545776367188\n",
            "Epoch 2/5, Batch 300, Loss: 3.5928032398223877\n",
            "Epoch 2/5, Batch 400, Loss: 3.633204460144043\n",
            "Epoch 2/5, Batch 500, Loss: 3.4709527492523193\n",
            "Epoch 2/5, Batch 600, Loss: 3.518953323364258\n",
            "Epoch 2/5, Batch 700, Loss: 3.566798210144043\n",
            "Epoch 2/5, Batch 800, Loss: 3.568612813949585\n",
            "Epoch 2/5, Batch 900, Loss: 3.493919610977173\n",
            "Epoch 2/5, Batch 1000, Loss: 3.462414026260376\n",
            "Epoch 2/5, Batch 1100, Loss: 3.5230770111083984\n",
            "Epoch 2/5, Batch 1200, Loss: 3.5190556049346924\n",
            "Epoch 2/5, Batch 1300, Loss: 3.4332361221313477\n",
            "Epoch 2/5, Batch 1400, Loss: 3.501957416534424\n",
            "Epoch 2/5, Batch 1500, Loss: 3.554155111312866\n",
            "Epoch 2/5, Batch 1600, Loss: 3.3852949142456055\n",
            "Epoch 2/5, Batch 1700, Loss: 3.5595881938934326\n",
            "Epoch 2/5, Batch 1800, Loss: 3.389805555343628\n",
            "Epoch 2/5, Batch 1900, Loss: 3.528468132019043\n",
            "Epoch 2/5, Batch 2000, Loss: 3.4312779903411865\n",
            "Epoch 2/5, Batch 2100, Loss: 3.520451784133911\n",
            "Epoch 2/5, Batch 2200, Loss: 3.3152105808258057\n",
            "Epoch 2/5, Batch 2300, Loss: 3.492694616317749\n",
            "Epoch 2/5, Batch 2400, Loss: 3.480926275253296\n",
            "Epoch 2/5, Batch 2500, Loss: 3.4417130947113037\n",
            "Epoch 2/5, Batch 2600, Loss: 3.4204635620117188\n",
            "Epoch 2/5, Batch 2700, Loss: 3.414217710494995\n",
            "Epoch 2/5, Batch 2800, Loss: 3.394627332687378\n",
            "Epoch 2/5, Batch 2900, Loss: 3.4232430458068848\n",
            "Epoch 2/5, Batch 3000, Loss: 3.4263784885406494\n",
            "Epoch 2/5, Batch 3100, Loss: 3.421207904815674\n",
            "Epoch 2/5, Batch 3200, Loss: 3.3120806217193604\n",
            "Epoch 2/5, Batch 3300, Loss: 3.2959847450256348\n",
            "Epoch 2/5, Batch 3400, Loss: 3.3031249046325684\n",
            "Epoch 2/5, Batch 3500, Loss: 3.4912824630737305\n",
            "Epoch 2/5, Batch 3600, Loss: 3.4047751426696777\n",
            "Epoch 2/5, Batch 3700, Loss: 3.3623290061950684\n",
            "Epoch 2/5, Batch 3800, Loss: 3.2906861305236816\n",
            "Epoch 2/5, Batch 3900, Loss: 3.433903217315674\n",
            "Epoch 2/5, Batch 4000, Loss: 3.353466749191284\n",
            "Epoch 2/5, Batch 4100, Loss: 3.288050651550293\n",
            "Epoch 2/5, Batch 4200, Loss: 3.310063362121582\n",
            "Epoch 2/5, Batch 4300, Loss: 3.238835334777832\n",
            "Epoch 2/5, Batch 4400, Loss: 3.3335983753204346\n",
            "Epoch 2/5, Batch 4500, Loss: 3.312096357345581\n",
            "Epoch 2/5, Batch 4600, Loss: 3.363464593887329\n",
            "Epoch 2/5, Batch 4700, Loss: 3.2935562133789062\n",
            "Epoch 2/5, Batch 4800, Loss: 3.3265106678009033\n",
            "Epoch 2/5, Batch 4900, Loss: 3.201277017593384\n",
            "Epoch 2/5, Batch 5000, Loss: 3.273871898651123\n",
            "Epoch 2/5, Batch 5100, Loss: 3.3181705474853516\n",
            "Epoch 2/5, Batch 5200, Loss: 3.3017232418060303\n",
            "Epoch 2/5, Batch 5300, Loss: 3.1983532905578613\n",
            "Epoch 2/5, Batch 5400, Loss: 3.3176889419555664\n",
            "Epoch 2/5, Batch 5500, Loss: 3.071474552154541\n",
            "Epoch 2/5, Batch 5600, Loss: 3.2216873168945312\n",
            "Epoch 2/5, Batch 5700, Loss: 3.2034308910369873\n",
            "Epoch 2/5, Batch 5800, Loss: 3.1784515380859375\n",
            "Epoch 2/5, Batch 5900, Loss: 3.2072880268096924\n",
            "Epoch 2/5, Batch 6000, Loss: 3.2183725833892822\n",
            "Epoch 2/5, Batch 6100, Loss: 3.2252697944641113\n",
            "Epoch 2/5, Batch 6200, Loss: 3.214064836502075\n",
            "Epoch 2/5, Batch 6300, Loss: 3.124188184738159\n",
            "Epoch 2/5, Batch 6400, Loss: 3.2156128883361816\n",
            "Epoch 2/5, Batch 6500, Loss: 3.1560676097869873\n",
            "Epoch 2/5, Batch 6600, Loss: 3.1566081047058105\n",
            "Epoch 2/5, Batch 6700, Loss: 3.2173125743865967\n",
            "Epoch 2/5, Batch 6800, Loss: 3.186657190322876\n",
            "Epoch 2/5, Batch 6900, Loss: 3.31339955329895\n",
            "Epoch 2/5, Batch 7000, Loss: 3.0793347358703613\n",
            "Epoch 2/5, Batch 7100, Loss: 3.0807182788848877\n",
            "Epoch 2/5, Batch 7200, Loss: 3.0859405994415283\n",
            "Epoch 2/5, Batch 7300, Loss: 3.1109728813171387\n",
            "Epoch 2/5, Batch 7400, Loss: 3.0229973793029785\n",
            "Epoch 2/5, Batch 7500, Loss: 3.1174521446228027\n",
            "Epoch 2/5, Batch 7600, Loss: 3.2391374111175537\n",
            "Epoch 2/5, Batch 7700, Loss: 3.1692216396331787\n",
            "Epoch 2/5, Batch 7800, Loss: 3.1737170219421387\n",
            "Epoch 2/5, Batch 7900, Loss: 3.1719489097595215\n",
            "Epoch 2/5, Batch 8000, Loss: 3.1201000213623047\n",
            "Epoch 2/5, Batch 8100, Loss: 2.9701356887817383\n",
            "Epoch 2/5, Batch 8200, Loss: 3.044599771499634\n",
            "Epoch 2/5, Batch 8300, Loss: 3.12260365486145\n",
            "Epoch 2/5, Batch 8400, Loss: 3.223767042160034\n",
            "Epoch 2/5, Batch 8500, Loss: 3.121537923812866\n",
            "Epoch 2/5, Batch 8600, Loss: 3.081536293029785\n",
            "Epoch 2/5, Batch 8700, Loss: 3.062899112701416\n",
            "Epoch 2/5, Batch 8800, Loss: 3.160815954208374\n",
            "Epoch 2/5, Batch 8900, Loss: 3.064674139022827\n",
            "Epoch 2/5, Batch 9000, Loss: 3.032116651535034\n",
            "Epoch 2/5, Batch 9100, Loss: 3.0149009227752686\n",
            "Epoch 2/5, Batch 9200, Loss: 2.9947822093963623\n",
            "Epoch 2/5, Batch 9300, Loss: 3.097228527069092\n",
            "Epoch 2/5, Batch 9400, Loss: 2.9524786472320557\n",
            "Epoch 2/5, Batch 9500, Loss: 3.1032822132110596\n",
            "Epoch 2/5, Batch 9600, Loss: 2.9675393104553223\n",
            "Epoch 2/5, Batch 9700, Loss: 3.006265640258789\n",
            "Epoch 2/5, Batch 9800, Loss: 3.0330355167388916\n",
            "Epoch 2/5, Batch 9900, Loss: 2.9937074184417725\n",
            "Epoch 2/5, Batch 10000, Loss: 2.9316632747650146\n",
            "Epoch 2/5, Batch 10100, Loss: 3.0206708908081055\n",
            "Epoch 2/5, Batch 10200, Loss: 3.083355665206909\n",
            "Epoch 2/5, Batch 10300, Loss: 2.9279983043670654\n",
            "Epoch 2/5, Batch 10400, Loss: 3.011596202850342\n",
            "Epoch 2/5, Batch 10500, Loss: 2.958198308944702\n",
            "Epoch 2/5, Batch 10600, Loss: 3.0512795448303223\n",
            "Epoch 2/5, Batch 10700, Loss: 3.0004515647888184\n",
            "Epoch 2/5, Batch 10800, Loss: 2.885765552520752\n",
            "Epoch 2/5, Batch 10900, Loss: 2.8754403591156006\n",
            "Epoch 2/5, Batch 11000, Loss: 2.9267537593841553\n",
            "Epoch 2/5, Batch 11100, Loss: 2.885082483291626\n",
            "Epoch 2/5, Batch 11200, Loss: 2.947502613067627\n",
            "Epoch 2/5, Batch 11300, Loss: 2.9073028564453125\n",
            "Epoch 2/5, Batch 11400, Loss: 3.025308609008789\n",
            "Epoch 2/5, Batch 11500, Loss: 2.7937471866607666\n",
            "Epoch 2/5, Batch 11600, Loss: 2.9751839637756348\n",
            "Epoch 2/5, Batch 11700, Loss: 2.855534553527832\n",
            "Epoch 2/5, Batch 11800, Loss: 2.962766170501709\n",
            "Epoch 2/5, Batch 11900, Loss: 2.9599108695983887\n",
            "Epoch 2/5, Batch 12000, Loss: 2.975956916809082\n",
            "Epoch 2/5, Batch 12100, Loss: 2.895298480987549\n",
            "Epoch 2/5, Batch 12200, Loss: 2.99312424659729\n",
            "Epoch 2/5, Batch 12300, Loss: 2.9785828590393066\n",
            "Epoch 2/5, Batch 12400, Loss: 2.903949022293091\n",
            "Epoch 2/5, Batch 12500, Loss: 2.876436471939087\n",
            "Epoch 2/5, Batch 12600, Loss: 2.9177420139312744\n",
            "Epoch 2/5, Batch 12700, Loss: 2.8582663536071777\n",
            "Epoch 2/5, Batch 12800, Loss: 2.9029784202575684\n",
            "Epoch 2/5, Batch 12900, Loss: 2.892773151397705\n",
            "Epoch 2/5, Batch 13000, Loss: 2.8013806343078613\n",
            "Epoch 2/5, Batch 13100, Loss: 2.8085777759552\n",
            "Epoch 2/5, Batch 13200, Loss: 2.8641836643218994\n",
            "Epoch 2/5, Batch 13300, Loss: 2.832721471786499\n",
            "Epoch 2/5, Batch 13400, Loss: 2.8627374172210693\n",
            "Epoch 2/5, Batch 13500, Loss: 2.828355312347412\n",
            "Epoch 2/5, Batch 13600, Loss: 2.8738996982574463\n",
            "Epoch 2/5, Batch 13700, Loss: 2.83413028717041\n",
            "Epoch 2/5, Batch 13800, Loss: 2.730938196182251\n",
            "Epoch 2/5, Batch 13900, Loss: 2.7993221282958984\n",
            "Epoch 2/5, Batch 14000, Loss: 2.835195779800415\n",
            "Epoch 2/5, Batch 14100, Loss: 2.8296761512756348\n",
            "Epoch 2/5, Batch 14200, Loss: 2.7540156841278076\n",
            "Epoch 2/5, Batch 14300, Loss: 2.79079532623291\n",
            "Epoch 2/5, Batch 14400, Loss: 2.7610585689544678\n",
            "Epoch 2/5, Batch 14500, Loss: 2.840790271759033\n",
            "Epoch 2/5, Batch 14600, Loss: 2.8070578575134277\n",
            "Epoch 2/5, Batch 14700, Loss: 2.922454357147217\n",
            "Epoch 2/5, Batch 14800, Loss: 2.690986394882202\n",
            "Epoch 2/5, Batch 14900, Loss: 2.779606819152832\n",
            "Epoch 2/5, Batch 15000, Loss: 2.6850788593292236\n",
            "Epoch 2/5, Batch 15100, Loss: 2.8035531044006348\n",
            "Epoch 2/5, Batch 15200, Loss: 2.757422685623169\n",
            "Epoch 2/5, Batch 15300, Loss: 2.739449977874756\n",
            "Epoch 2/5, Batch 15400, Loss: 2.645050048828125\n",
            "Epoch 2/5, Batch 15500, Loss: 2.63094425201416\n",
            "Epoch 2/5, Batch 15600, Loss: 2.7976064682006836\n",
            "Epoch 2/5, Batch 15700, Loss: 2.6302905082702637\n",
            "Epoch 2/5, Batch 15800, Loss: 2.7601354122161865\n",
            "Epoch 2/5, Batch 15900, Loss: 2.8214521408081055\n",
            "Epoch 2/5, Batch 16000, Loss: 2.7329611778259277\n",
            "Epoch 2/5, Batch 16100, Loss: 2.804013729095459\n",
            "Epoch 2/5, Batch 16200, Loss: 2.7392399311065674\n",
            "Epoch 2/5, Batch 16300, Loss: 2.776010751724243\n",
            "Epoch [2/5], Average Loss: 3.114129688751845\n",
            "Epoch 3/5, Batch 0, Loss: 2.615424394607544\n",
            "Epoch 3/5, Batch 100, Loss: 2.641770839691162\n",
            "Epoch 3/5, Batch 200, Loss: 2.6501362323760986\n",
            "Epoch 3/5, Batch 300, Loss: 2.743645668029785\n",
            "Epoch 3/5, Batch 400, Loss: 2.6738600730895996\n",
            "Epoch 3/5, Batch 500, Loss: 2.6374144554138184\n",
            "Epoch 3/5, Batch 600, Loss: 2.707534074783325\n",
            "Epoch 3/5, Batch 700, Loss: 2.6550629138946533\n",
            "Epoch 3/5, Batch 800, Loss: 2.614920139312744\n",
            "Epoch 3/5, Batch 900, Loss: 2.539395332336426\n",
            "Epoch 3/5, Batch 1000, Loss: 2.580319881439209\n",
            "Epoch 3/5, Batch 1100, Loss: 2.603654623031616\n",
            "Epoch 3/5, Batch 1200, Loss: 2.602973699569702\n",
            "Epoch 3/5, Batch 1300, Loss: 2.6635584831237793\n",
            "Epoch 3/5, Batch 1400, Loss: 2.6316933631896973\n",
            "Epoch 3/5, Batch 1500, Loss: 2.648340940475464\n",
            "Epoch 3/5, Batch 1600, Loss: 2.5928308963775635\n",
            "Epoch 3/5, Batch 1700, Loss: 2.5960657596588135\n",
            "Epoch 3/5, Batch 1800, Loss: 2.6837446689605713\n",
            "Epoch 3/5, Batch 1900, Loss: 2.501033067703247\n",
            "Epoch 3/5, Batch 2000, Loss: 2.5810093879699707\n",
            "Epoch 3/5, Batch 2100, Loss: 2.6997482776641846\n",
            "Epoch 3/5, Batch 2200, Loss: 2.596691846847534\n",
            "Epoch 3/5, Batch 2300, Loss: 2.4852020740509033\n",
            "Epoch 3/5, Batch 2400, Loss: 2.586203098297119\n",
            "Epoch 3/5, Batch 2500, Loss: 2.5962135791778564\n",
            "Epoch 3/5, Batch 2600, Loss: 2.5189805030822754\n",
            "Epoch 3/5, Batch 2700, Loss: 2.643977403640747\n",
            "Epoch 3/5, Batch 2800, Loss: 2.581934690475464\n",
            "Epoch 3/5, Batch 2900, Loss: 2.5475518703460693\n",
            "Epoch 3/5, Batch 3000, Loss: 2.701789379119873\n",
            "Epoch 3/5, Batch 3100, Loss: 2.544304132461548\n",
            "Epoch 3/5, Batch 3200, Loss: 2.642786979675293\n",
            "Epoch 3/5, Batch 3300, Loss: 2.663144111633301\n",
            "Epoch 3/5, Batch 3400, Loss: 2.56457781791687\n",
            "Epoch 3/5, Batch 3500, Loss: 2.6607871055603027\n",
            "Epoch 3/5, Batch 3600, Loss: 2.5683786869049072\n",
            "Epoch 3/5, Batch 3700, Loss: 2.6494064331054688\n",
            "Epoch 3/5, Batch 3800, Loss: 2.6264805793762207\n",
            "Epoch 3/5, Batch 3900, Loss: 2.6002180576324463\n",
            "Epoch 3/5, Batch 4000, Loss: 2.542039632797241\n",
            "Epoch 3/5, Batch 4100, Loss: 2.5877227783203125\n",
            "Epoch 3/5, Batch 4200, Loss: 2.5919389724731445\n",
            "Epoch 3/5, Batch 4300, Loss: 2.580209255218506\n",
            "Epoch 3/5, Batch 4400, Loss: 2.630124568939209\n",
            "Epoch 3/5, Batch 4500, Loss: 2.579832077026367\n",
            "Epoch 3/5, Batch 4600, Loss: 2.575735330581665\n",
            "Epoch 3/5, Batch 4700, Loss: 2.61936092376709\n",
            "Epoch 3/5, Batch 4800, Loss: 2.592621326446533\n",
            "Epoch 3/5, Batch 4900, Loss: 2.5372049808502197\n",
            "Epoch 3/5, Batch 5000, Loss: 2.5398478507995605\n",
            "Epoch 3/5, Batch 5100, Loss: 2.5978145599365234\n",
            "Epoch 3/5, Batch 5200, Loss: 2.572296142578125\n",
            "Epoch 3/5, Batch 5300, Loss: 2.3994081020355225\n",
            "Epoch 3/5, Batch 5400, Loss: 2.5448763370513916\n",
            "Epoch 3/5, Batch 5500, Loss: 2.528787612915039\n",
            "Epoch 3/5, Batch 5600, Loss: 2.4181597232818604\n",
            "Epoch 3/5, Batch 5700, Loss: 2.5240859985351562\n",
            "Epoch 3/5, Batch 5800, Loss: 2.4550552368164062\n",
            "Epoch 3/5, Batch 5900, Loss: 2.505446672439575\n",
            "Epoch 3/5, Batch 6000, Loss: 2.4459426403045654\n",
            "Epoch 3/5, Batch 6100, Loss: 2.5187489986419678\n",
            "Epoch 3/5, Batch 6200, Loss: 2.494279384613037\n",
            "Epoch 3/5, Batch 6300, Loss: 2.5492777824401855\n",
            "Epoch 3/5, Batch 6400, Loss: 2.582956075668335\n",
            "Epoch 3/5, Batch 6500, Loss: 2.4986066818237305\n",
            "Epoch 3/5, Batch 6600, Loss: 2.44136381149292\n",
            "Epoch 3/5, Batch 6700, Loss: 2.4628000259399414\n",
            "Epoch 3/5, Batch 6800, Loss: 2.3694350719451904\n",
            "Epoch 3/5, Batch 6900, Loss: 2.5092451572418213\n",
            "Epoch 3/5, Batch 7000, Loss: 2.437917470932007\n",
            "Epoch 3/5, Batch 7100, Loss: 2.467437982559204\n",
            "Epoch 3/5, Batch 7200, Loss: 2.524918556213379\n",
            "Epoch 3/5, Batch 7300, Loss: 2.46254825592041\n",
            "Epoch 3/5, Batch 7400, Loss: 2.4726154804229736\n",
            "Epoch 3/5, Batch 7500, Loss: 2.5533804893493652\n",
            "Epoch 3/5, Batch 7600, Loss: 2.4456708431243896\n",
            "Epoch 3/5, Batch 7700, Loss: 2.4932734966278076\n",
            "Epoch 3/5, Batch 7800, Loss: 2.342294931411743\n",
            "Epoch 3/5, Batch 7900, Loss: 2.4872190952301025\n",
            "Epoch 3/5, Batch 8000, Loss: 2.3851826190948486\n",
            "Epoch 3/5, Batch 8100, Loss: 2.39905047416687\n",
            "Epoch 3/5, Batch 8200, Loss: 2.5065393447875977\n",
            "Epoch 3/5, Batch 8300, Loss: 2.330167055130005\n",
            "Epoch 3/5, Batch 8400, Loss: 2.418334722518921\n",
            "Epoch 3/5, Batch 8500, Loss: 2.4398701190948486\n",
            "Epoch 3/5, Batch 8600, Loss: 2.3803956508636475\n",
            "Epoch 3/5, Batch 8700, Loss: 2.2941811084747314\n",
            "Epoch 3/5, Batch 8800, Loss: 2.3913354873657227\n",
            "Epoch 3/5, Batch 8900, Loss: 2.3535377979278564\n",
            "Epoch 3/5, Batch 9000, Loss: 2.449601173400879\n",
            "Epoch 3/5, Batch 9100, Loss: 2.405534267425537\n",
            "Epoch 3/5, Batch 9200, Loss: 2.3189845085144043\n",
            "Epoch 3/5, Batch 9300, Loss: 2.4774696826934814\n",
            "Epoch 3/5, Batch 9400, Loss: 2.448824644088745\n",
            "Epoch 3/5, Batch 9500, Loss: 2.368995189666748\n",
            "Epoch 3/5, Batch 9600, Loss: 2.486572504043579\n",
            "Epoch 3/5, Batch 9700, Loss: 2.3873167037963867\n",
            "Epoch 3/5, Batch 9800, Loss: 2.4574265480041504\n",
            "Epoch 3/5, Batch 9900, Loss: 2.4256083965301514\n",
            "Epoch 3/5, Batch 10000, Loss: 2.475151538848877\n",
            "Epoch 3/5, Batch 10100, Loss: 2.4702930450439453\n",
            "Epoch 3/5, Batch 10200, Loss: 2.407996654510498\n",
            "Epoch 3/5, Batch 10300, Loss: 2.3566300868988037\n",
            "Epoch 3/5, Batch 10400, Loss: 2.3192522525787354\n",
            "Epoch 3/5, Batch 10500, Loss: 2.3042778968811035\n",
            "Epoch 3/5, Batch 10600, Loss: 2.3715763092041016\n",
            "Epoch 3/5, Batch 10700, Loss: 2.3043360710144043\n",
            "Epoch 3/5, Batch 10800, Loss: 2.4466145038604736\n",
            "Epoch 3/5, Batch 10900, Loss: 2.349525213241577\n",
            "Epoch 3/5, Batch 11000, Loss: 2.3785767555236816\n",
            "Epoch 3/5, Batch 11100, Loss: 2.330958366394043\n",
            "Epoch 3/5, Batch 11200, Loss: 2.3406903743743896\n",
            "Epoch 3/5, Batch 11300, Loss: 2.3976004123687744\n",
            "Epoch 3/5, Batch 11400, Loss: 2.3540046215057373\n",
            "Epoch 3/5, Batch 11500, Loss: 2.3690738677978516\n",
            "Epoch 3/5, Batch 11600, Loss: 2.312743663787842\n",
            "Epoch 3/5, Batch 11700, Loss: 2.431579351425171\n",
            "Epoch 3/5, Batch 11800, Loss: 2.3799378871917725\n",
            "Epoch 3/5, Batch 11900, Loss: 2.3775863647460938\n",
            "Epoch 3/5, Batch 12000, Loss: 2.3284220695495605\n",
            "Epoch 3/5, Batch 12100, Loss: 2.362051248550415\n",
            "Epoch 3/5, Batch 12200, Loss: 2.2277567386627197\n",
            "Epoch 3/5, Batch 12300, Loss: 2.209685802459717\n",
            "Epoch 3/5, Batch 12400, Loss: 2.4341912269592285\n",
            "Epoch 3/5, Batch 12500, Loss: 2.4065678119659424\n",
            "Epoch 3/5, Batch 12600, Loss: 2.317385196685791\n",
            "Epoch 3/5, Batch 12700, Loss: 2.303309440612793\n",
            "Epoch 3/5, Batch 12800, Loss: 2.37032151222229\n",
            "Epoch 3/5, Batch 12900, Loss: 2.412057876586914\n",
            "Epoch 3/5, Batch 13000, Loss: 2.270613193511963\n",
            "Epoch 3/5, Batch 13100, Loss: 2.3905999660491943\n",
            "Epoch 3/5, Batch 13200, Loss: 2.4421122074127197\n",
            "Epoch 3/5, Batch 13300, Loss: 2.227114200592041\n",
            "Epoch 3/5, Batch 13400, Loss: 2.2686049938201904\n",
            "Epoch 3/5, Batch 13500, Loss: 2.374488115310669\n",
            "Epoch 3/5, Batch 13600, Loss: 2.2577316761016846\n",
            "Epoch 3/5, Batch 13700, Loss: 2.4157605171203613\n",
            "Epoch 3/5, Batch 13800, Loss: 2.2411439418792725\n",
            "Epoch 3/5, Batch 13900, Loss: 2.305211305618286\n",
            "Epoch 3/5, Batch 14000, Loss: 2.1808412075042725\n",
            "Epoch 3/5, Batch 14100, Loss: 2.242882013320923\n",
            "Epoch 3/5, Batch 14200, Loss: 2.3859472274780273\n",
            "Epoch 3/5, Batch 14300, Loss: 2.3204545974731445\n",
            "Epoch 3/5, Batch 14400, Loss: 2.3536431789398193\n",
            "Epoch 3/5, Batch 14500, Loss: 2.3125288486480713\n",
            "Epoch 3/5, Batch 14600, Loss: 2.2131552696228027\n",
            "Epoch 3/5, Batch 14700, Loss: 2.2896857261657715\n",
            "Epoch 3/5, Batch 14800, Loss: 2.215996503829956\n",
            "Epoch 3/5, Batch 14900, Loss: 2.318448066711426\n",
            "Epoch 3/5, Batch 15000, Loss: 2.3483760356903076\n",
            "Epoch 3/5, Batch 15100, Loss: 2.3972630500793457\n",
            "Epoch 3/5, Batch 15200, Loss: 2.315095901489258\n",
            "Epoch 3/5, Batch 15300, Loss: 2.3053107261657715\n",
            "Epoch 3/5, Batch 15400, Loss: 2.2735512256622314\n",
            "Epoch 3/5, Batch 15500, Loss: 2.2444047927856445\n",
            "Epoch 3/5, Batch 15600, Loss: 2.407036066055298\n",
            "Epoch 3/5, Batch 15700, Loss: 2.358158588409424\n",
            "Epoch 3/5, Batch 15800, Loss: 2.283294200897217\n",
            "Epoch 3/5, Batch 15900, Loss: 2.2456228733062744\n",
            "Epoch 3/5, Batch 16000, Loss: 2.286799669265747\n",
            "Epoch 3/5, Batch 16100, Loss: 2.2743821144104004\n",
            "Epoch 3/5, Batch 16200, Loss: 2.2884316444396973\n",
            "Epoch 3/5, Batch 16300, Loss: 2.407423257827759\n",
            "Epoch [3/5], Average Loss: 2.4524827756088974\n",
            "Epoch 4/5, Batch 0, Loss: 2.2202954292297363\n",
            "Epoch 4/5, Batch 100, Loss: 2.237743616104126\n",
            "Epoch 4/5, Batch 200, Loss: 2.1258652210235596\n",
            "Epoch 4/5, Batch 300, Loss: 2.2966842651367188\n",
            "Epoch 4/5, Batch 400, Loss: 2.236590623855591\n",
            "Epoch 4/5, Batch 500, Loss: 2.2376902103424072\n",
            "Epoch 4/5, Batch 600, Loss: 2.2102723121643066\n",
            "Epoch 4/5, Batch 700, Loss: 2.1299855709075928\n",
            "Epoch 4/5, Batch 800, Loss: 2.168779134750366\n",
            "Epoch 4/5, Batch 900, Loss: 2.296663284301758\n",
            "Epoch 4/5, Batch 1000, Loss: 2.0586559772491455\n",
            "Epoch 4/5, Batch 1100, Loss: 2.273132801055908\n",
            "Epoch 4/5, Batch 1200, Loss: 2.1808876991271973\n",
            "Epoch 4/5, Batch 1300, Loss: 2.2375710010528564\n",
            "Epoch 4/5, Batch 1400, Loss: 2.133233070373535\n",
            "Epoch 4/5, Batch 1500, Loss: 2.202031135559082\n",
            "Epoch 4/5, Batch 1600, Loss: 2.140324592590332\n",
            "Epoch 4/5, Batch 1700, Loss: 2.145338535308838\n",
            "Epoch 4/5, Batch 1800, Loss: 2.2184011936187744\n",
            "Epoch 4/5, Batch 1900, Loss: 2.1786913871765137\n",
            "Epoch 4/5, Batch 2000, Loss: 2.1684272289276123\n",
            "Epoch 4/5, Batch 2100, Loss: 2.216827392578125\n",
            "Epoch 4/5, Batch 2200, Loss: 2.1732020378112793\n",
            "Epoch 4/5, Batch 2300, Loss: 2.0883233547210693\n",
            "Epoch 4/5, Batch 2400, Loss: 2.2153329849243164\n",
            "Epoch 4/5, Batch 2500, Loss: 2.2095837593078613\n",
            "Epoch 4/5, Batch 2600, Loss: 2.1507084369659424\n",
            "Epoch 4/5, Batch 2700, Loss: 2.068053722381592\n",
            "Epoch 4/5, Batch 2800, Loss: 2.216055154800415\n",
            "Epoch 4/5, Batch 2900, Loss: 2.0870471000671387\n",
            "Epoch 4/5, Batch 3000, Loss: 2.2242751121520996\n",
            "Epoch 4/5, Batch 3100, Loss: 2.1300458908081055\n",
            "Epoch 4/5, Batch 3200, Loss: 2.19840145111084\n",
            "Epoch 4/5, Batch 3300, Loss: 2.117016553878784\n",
            "Epoch 4/5, Batch 3400, Loss: 2.157768487930298\n",
            "Epoch 4/5, Batch 3500, Loss: 2.2118146419525146\n",
            "Epoch 4/5, Batch 3600, Loss: 2.0245962142944336\n",
            "Epoch 4/5, Batch 3700, Loss: 2.1362810134887695\n",
            "Epoch 4/5, Batch 3800, Loss: 2.1463985443115234\n",
            "Epoch 4/5, Batch 3900, Loss: 2.1750574111938477\n",
            "Epoch 4/5, Batch 4000, Loss: 2.237499952316284\n",
            "Epoch 4/5, Batch 4100, Loss: 2.1677708625793457\n",
            "Epoch 4/5, Batch 4200, Loss: 2.1880311965942383\n",
            "Epoch 4/5, Batch 4300, Loss: 2.1434736251831055\n",
            "Epoch 4/5, Batch 4400, Loss: 2.129478693008423\n",
            "Epoch 4/5, Batch 4500, Loss: 2.231950521469116\n",
            "Epoch 4/5, Batch 4600, Loss: 2.1589701175689697\n",
            "Epoch 4/5, Batch 4700, Loss: 2.030717611312866\n",
            "Epoch 4/5, Batch 4800, Loss: 2.1814823150634766\n",
            "Epoch 4/5, Batch 4900, Loss: 2.283280372619629\n",
            "Epoch 4/5, Batch 5000, Loss: 2.2290008068084717\n",
            "Epoch 4/5, Batch 5100, Loss: 2.1652207374572754\n",
            "Epoch 4/5, Batch 5200, Loss: 2.154430389404297\n",
            "Epoch 4/5, Batch 5300, Loss: 2.217881202697754\n",
            "Epoch 4/5, Batch 5400, Loss: 2.2268524169921875\n",
            "Epoch 4/5, Batch 5500, Loss: 2.0817458629608154\n",
            "Epoch 4/5, Batch 5600, Loss: 2.161996603012085\n",
            "Epoch 4/5, Batch 5700, Loss: 2.071925640106201\n",
            "Epoch 4/5, Batch 5800, Loss: 1.9906340837478638\n",
            "Epoch 4/5, Batch 5900, Loss: 2.1262125968933105\n",
            "Epoch 4/5, Batch 6000, Loss: 2.080580949783325\n",
            "Epoch 4/5, Batch 6100, Loss: 2.038562059402466\n",
            "Epoch 4/5, Batch 6200, Loss: 2.210383653640747\n",
            "Epoch 4/5, Batch 6300, Loss: 2.104160785675049\n",
            "Epoch 4/5, Batch 6400, Loss: 2.1570374965667725\n",
            "Epoch 4/5, Batch 6500, Loss: 2.212618350982666\n",
            "Epoch 4/5, Batch 6600, Loss: 2.135195732116699\n",
            "Epoch 4/5, Batch 6700, Loss: 2.097351551055908\n",
            "Epoch 4/5, Batch 6800, Loss: 2.074185848236084\n",
            "Epoch 4/5, Batch 6900, Loss: 1.9903490543365479\n",
            "Epoch 4/5, Batch 7000, Loss: 2.1359894275665283\n",
            "Epoch 4/5, Batch 7100, Loss: 2.201869249343872\n",
            "Epoch 4/5, Batch 7200, Loss: 2.158374786376953\n",
            "Epoch 4/5, Batch 7300, Loss: 2.1066720485687256\n",
            "Epoch 4/5, Batch 7400, Loss: 2.039478063583374\n",
            "Epoch 4/5, Batch 7500, Loss: 2.0887069702148438\n",
            "Epoch 4/5, Batch 7600, Loss: 2.1850082874298096\n",
            "Epoch 4/5, Batch 7700, Loss: 2.1665451526641846\n",
            "Epoch 4/5, Batch 7800, Loss: 2.0870842933654785\n",
            "Epoch 4/5, Batch 7900, Loss: 2.1303398609161377\n",
            "Epoch 4/5, Batch 8000, Loss: 2.0152037143707275\n",
            "Epoch 4/5, Batch 8100, Loss: 2.0106818675994873\n",
            "Epoch 4/5, Batch 8200, Loss: 2.136106252670288\n",
            "Epoch 4/5, Batch 8300, Loss: 2.059124708175659\n",
            "Epoch 4/5, Batch 8400, Loss: 2.1123206615448\n",
            "Epoch 4/5, Batch 8500, Loss: 2.120004177093506\n",
            "Epoch 4/5, Batch 8600, Loss: 2.0366947650909424\n",
            "Epoch 4/5, Batch 8700, Loss: 2.190518856048584\n",
            "Epoch 4/5, Batch 8800, Loss: 2.139737844467163\n",
            "Epoch 4/5, Batch 8900, Loss: 2.1263556480407715\n",
            "Epoch 4/5, Batch 9000, Loss: 2.0508298873901367\n",
            "Epoch 4/5, Batch 9100, Loss: 2.2277097702026367\n",
            "Epoch 4/5, Batch 9200, Loss: 2.105456829071045\n",
            "Epoch 4/5, Batch 9300, Loss: 1.9129077196121216\n",
            "Epoch 4/5, Batch 9400, Loss: 2.081511974334717\n",
            "Epoch 4/5, Batch 9500, Loss: 2.0436060428619385\n",
            "Epoch 4/5, Batch 9600, Loss: 2.0473647117614746\n",
            "Epoch 4/5, Batch 9700, Loss: 2.1174635887145996\n",
            "Epoch 4/5, Batch 9800, Loss: 2.0554444789886475\n",
            "Epoch 4/5, Batch 9900, Loss: 2.0647037029266357\n",
            "Epoch 4/5, Batch 10000, Loss: 2.04079532623291\n",
            "Epoch 4/5, Batch 10100, Loss: 2.006948709487915\n",
            "Epoch 4/5, Batch 10200, Loss: 2.153672456741333\n",
            "Epoch 4/5, Batch 10300, Loss: 2.1269450187683105\n",
            "Epoch 4/5, Batch 10400, Loss: 2.027845621109009\n",
            "Epoch 4/5, Batch 10500, Loss: 2.1709065437316895\n",
            "Epoch 4/5, Batch 10600, Loss: 1.9774457216262817\n",
            "Epoch 4/5, Batch 10700, Loss: 2.0851879119873047\n",
            "Epoch 4/5, Batch 10800, Loss: 2.0485680103302\n",
            "Epoch 4/5, Batch 10900, Loss: 2.0598607063293457\n",
            "Epoch 4/5, Batch 11000, Loss: 2.2256388664245605\n",
            "Epoch 4/5, Batch 11100, Loss: 2.0592305660247803\n",
            "Epoch 4/5, Batch 11200, Loss: 2.072039842605591\n",
            "Epoch 4/5, Batch 11300, Loss: 2.1268961429595947\n",
            "Epoch 4/5, Batch 11400, Loss: 2.03305983543396\n",
            "Epoch 4/5, Batch 11500, Loss: 2.011141061782837\n",
            "Epoch 4/5, Batch 11600, Loss: 2.0677640438079834\n",
            "Epoch 4/5, Batch 11700, Loss: 1.982590675354004\n",
            "Epoch 4/5, Batch 11800, Loss: 1.9982964992523193\n",
            "Epoch 4/5, Batch 11900, Loss: 1.96249520778656\n",
            "Epoch 4/5, Batch 12000, Loss: 2.1010637283325195\n",
            "Epoch 4/5, Batch 12100, Loss: 2.104119300842285\n",
            "Epoch 4/5, Batch 12200, Loss: 2.061574935913086\n",
            "Epoch 4/5, Batch 12300, Loss: 2.058025360107422\n",
            "Epoch 4/5, Batch 12400, Loss: 2.0189154148101807\n",
            "Epoch 4/5, Batch 12500, Loss: 1.934088110923767\n",
            "Epoch 4/5, Batch 12600, Loss: 2.0310049057006836\n",
            "Epoch 4/5, Batch 12700, Loss: 2.0228428840637207\n",
            "Epoch 4/5, Batch 12800, Loss: 2.080075740814209\n",
            "Epoch 4/5, Batch 12900, Loss: 2.0135579109191895\n",
            "Epoch 4/5, Batch 13000, Loss: 1.945034384727478\n",
            "Epoch 4/5, Batch 13100, Loss: 2.03774094581604\n",
            "Epoch 4/5, Batch 13200, Loss: 2.0651025772094727\n",
            "Epoch 4/5, Batch 13300, Loss: 2.023122787475586\n",
            "Epoch 4/5, Batch 13400, Loss: 2.093688726425171\n",
            "Epoch 4/5, Batch 13500, Loss: 2.045240640640259\n",
            "Epoch 4/5, Batch 13600, Loss: 2.004133462905884\n",
            "Epoch 4/5, Batch 13700, Loss: 1.9138762950897217\n",
            "Epoch 4/5, Batch 13800, Loss: 2.057274103164673\n",
            "Epoch 4/5, Batch 13900, Loss: 2.069930076599121\n",
            "Epoch 4/5, Batch 14000, Loss: 2.0064735412597656\n",
            "Epoch 4/5, Batch 14100, Loss: 2.125652551651001\n",
            "Epoch 4/5, Batch 14200, Loss: 2.0152690410614014\n",
            "Epoch 4/5, Batch 14300, Loss: 2.1415135860443115\n",
            "Epoch 4/5, Batch 14400, Loss: 2.0075583457946777\n",
            "Epoch 4/5, Batch 14500, Loss: 2.0279839038848877\n",
            "Epoch 4/5, Batch 14600, Loss: 2.0630147457122803\n",
            "Epoch 4/5, Batch 14700, Loss: 2.0088953971862793\n",
            "Epoch 4/5, Batch 14800, Loss: 2.125004291534424\n",
            "Epoch 4/5, Batch 14900, Loss: 2.077819347381592\n",
            "Epoch 4/5, Batch 15000, Loss: 2.011126756668091\n",
            "Epoch 4/5, Batch 15100, Loss: 1.9971840381622314\n",
            "Epoch 4/5, Batch 15200, Loss: 2.0938103199005127\n",
            "Epoch 4/5, Batch 15300, Loss: 1.9779797792434692\n",
            "Epoch 4/5, Batch 15400, Loss: 2.014524221420288\n",
            "Epoch 4/5, Batch 15500, Loss: 2.0519182682037354\n",
            "Epoch 4/5, Batch 15600, Loss: 1.938387393951416\n",
            "Epoch 4/5, Batch 15700, Loss: 1.951326608657837\n",
            "Epoch 4/5, Batch 15800, Loss: 1.9537959098815918\n",
            "Epoch 4/5, Batch 15900, Loss: 1.9962705373764038\n",
            "Epoch 4/5, Batch 16000, Loss: 2.0220563411712646\n",
            "Epoch 4/5, Batch 16100, Loss: 2.05521559715271\n",
            "Epoch 4/5, Batch 16200, Loss: 2.116178035736084\n",
            "Epoch 4/5, Batch 16300, Loss: 2.0929532051086426\n",
            "Epoch [4/5], Average Loss: 2.104220750194181\n",
            "Epoch 5/5, Batch 0, Loss: 2.001256227493286\n",
            "Epoch 5/5, Batch 100, Loss: 1.919541597366333\n",
            "Epoch 5/5, Batch 200, Loss: 1.8621965646743774\n",
            "Epoch 5/5, Batch 300, Loss: 1.9496111869812012\n",
            "Epoch 5/5, Batch 400, Loss: 2.0213944911956787\n",
            "Epoch 5/5, Batch 500, Loss: 1.8765623569488525\n",
            "Epoch 5/5, Batch 600, Loss: 1.9954122304916382\n",
            "Epoch 5/5, Batch 700, Loss: 1.9850691556930542\n",
            "Epoch 5/5, Batch 800, Loss: 1.977321743965149\n",
            "Epoch 5/5, Batch 900, Loss: 1.9690029621124268\n",
            "Epoch 5/5, Batch 1000, Loss: 2.0420055389404297\n",
            "Epoch 5/5, Batch 1100, Loss: 2.009363889694214\n",
            "Epoch 5/5, Batch 1200, Loss: 1.994220495223999\n",
            "Epoch 5/5, Batch 1300, Loss: 1.936209797859192\n",
            "Epoch 5/5, Batch 1400, Loss: 1.9171555042266846\n",
            "Epoch 5/5, Batch 1500, Loss: 2.0274531841278076\n",
            "Epoch 5/5, Batch 1600, Loss: 1.9690476655960083\n",
            "Epoch 5/5, Batch 1700, Loss: 1.8899281024932861\n",
            "Epoch 5/5, Batch 1800, Loss: 1.9827685356140137\n",
            "Epoch 5/5, Batch 1900, Loss: 1.9450174570083618\n",
            "Epoch 5/5, Batch 2000, Loss: 1.8723251819610596\n",
            "Epoch 5/5, Batch 2100, Loss: 1.981961965560913\n",
            "Epoch 5/5, Batch 2200, Loss: 1.911063313484192\n",
            "Epoch 5/5, Batch 2300, Loss: 1.9959813356399536\n",
            "Epoch 5/5, Batch 2400, Loss: 1.927141785621643\n",
            "Epoch 5/5, Batch 2500, Loss: 1.9331988096237183\n",
            "Epoch 5/5, Batch 2600, Loss: 1.9279649257659912\n",
            "Epoch 5/5, Batch 2700, Loss: 1.8961960077285767\n",
            "Epoch 5/5, Batch 2800, Loss: 1.925889492034912\n",
            "Epoch 5/5, Batch 2900, Loss: 1.9154160022735596\n",
            "Epoch 5/5, Batch 3000, Loss: 1.963402271270752\n",
            "Epoch 5/5, Batch 3100, Loss: 1.9283101558685303\n",
            "Epoch 5/5, Batch 3200, Loss: 1.930038332939148\n",
            "Epoch 5/5, Batch 3300, Loss: 1.9090741872787476\n",
            "Epoch 5/5, Batch 3400, Loss: 1.9487926959991455\n",
            "Epoch 5/5, Batch 3500, Loss: 1.874495029449463\n",
            "Epoch 5/5, Batch 3600, Loss: 1.8395555019378662\n",
            "Epoch 5/5, Batch 3700, Loss: 1.981522560119629\n",
            "Epoch 5/5, Batch 3800, Loss: 1.9098693132400513\n",
            "Epoch 5/5, Batch 3900, Loss: 2.013805627822876\n",
            "Epoch 5/5, Batch 4000, Loss: 1.8806054592132568\n",
            "Epoch 5/5, Batch 4100, Loss: 1.9324326515197754\n",
            "Epoch 5/5, Batch 4200, Loss: 1.9913711547851562\n",
            "Epoch 5/5, Batch 4300, Loss: 1.9270052909851074\n",
            "Epoch 5/5, Batch 4400, Loss: 1.8652637004852295\n",
            "Epoch 5/5, Batch 4500, Loss: 1.9261871576309204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "oUqOuKsOw84s"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "def load_model(model_path, vocab_size, embedding_dim, hidden_size, num_layers):\n",
        "    # Initialize the model\n",
        "    lstm_model = init_model(vocab_size, embedding_dim, hidden_size, num_layers)\n",
        "    # Load the trained model parameters\n",
        "    lstm_model.load_state_dict(torch.load(model_path))\n",
        "    lstm_model.eval()  # Set the model to evaluation mode\n",
        "    return lstm_model\n",
        "\n",
        "def tokenize_input(input_text, token_to_index):\n",
        "    # Tokenize the input text and convert to indices\n",
        "    tokens = tokenize_text(input_text, use_nltk=True)\n",
        "    token_indices = [token_to_index.get(token, 0) for token in tokens]  # Unknown tokens as 0\n",
        "    return token_indices\n",
        "\n",
        "def predict_next_tokens(model, input_indices, num_predictions, vocab_size):\n",
        "    model.eval()  # Ensure model is in evaluation mode\n",
        "    predictions = []\n",
        "    input_tensor = torch.tensor(input_indices).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    for _ in range(num_predictions):\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "            last_token_logits = output[0, -1, :]\n",
        "            predicted_token_id = torch.argmax(last_token_logits, dim=-1).item()\n",
        "            predictions.append(predicted_token_id)\n",
        "            input_tensor = torch.cat([input_tensor[0], torch.tensor([predicted_token_id])]).unsqueeze(0)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Model Hyperparameters (should match training configuration)\n",
        "vocab_size = 76538  # Replace with your actual vocab size\n",
        "embedding_dim = 256\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "model_path = 'trained_lstm_model.pth'\n",
        "\n",
        "# Load token_to_index and index_to_token\n",
        "with open('token_to_index.pkl', 'rb') as f:\n",
        "    token_to_index = pickle.load(f)\n",
        "\n",
        "with open('index_to_token.pkl', 'rb') as f:\n",
        "    index_to_token = pickle.load(f)\n",
        "\n",
        "# Load the trained model\n",
        "lstm_model = load_model(model_path, vocab_size, embedding_dim, hidden_size, num_layers)\n",
        "\n",
        "# Example input text\n",
        "# input_text = \"Senj no Valkyria 3 : Unrecorded\"\n",
        "input_text = \"Kennedy is the president of: \"\n",
        "\n",
        "input_indices = tokenize_input(input_text, token_to_index)  # token_to_index from your training script\n",
        "\n",
        "# Predict the next 5 tokens\n",
        "num_predictions = 10\n",
        "predicted_indices = predict_next_tokens(lstm_model, input_indices, num_predictions, vocab_size)\n",
        "\n",
        "# Convert indices to tokens\n",
        "predicted_tokens = [index_to_token[idx] for idx in predicted_indices]  # index_to_token from your training script\n",
        "\n",
        "print(\"Predicted tokens:\", predicted_tokens)\n"
      ],
      "metadata": {
        "id": "lsU3d10etDsT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}